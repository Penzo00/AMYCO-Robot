{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6297,
     "status": "ok",
     "timestamp": 1722271979600,
     "user": {
      "displayName": "Lorenzo Vergata",
      "userId": "01696189899843508265"
     },
     "user_tz": -120
    },
    "id": "7NCPXoXzIDHY",
    "outputId": "6977a8c7-a78a-4aa2-e72b-aba61c734729",
    "ExecuteTime": {
     "end_time": "2024-11-18T16:10:15.482538Z",
     "start_time": "2024-11-18T16:10:08.703705Z"
    }
   },
   "source": [
    "# Fix randomness and hide warnings\n",
    "seed = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Importa il modello FasterViT\n",
    "from keras_cv_attention_models import fastervit\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731946209.677853  502208 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731946209.699161  502208 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4070 Ti SUPER, compute capability 8.9\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_dir = \"MO2/classes\"\n",
    "batch_size = 64\n",
    "final_image_size = (309, 298)\n",
    "\n",
    "# Carica tutte le immagini dalle directory \"classes\" e \"augmented\"\n",
    "train_ds = tfk.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    image_size=final_image_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    label_mode='int',\n",
    "    seed=seed)\n",
    "\n",
    "data_dir = \"MO2/augmented_data\"\n",
    "\n",
    "# Carica tutte le immagini dalle directory \"classes\" e \"augmented\"\n",
    "train_ds2 = tfk.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    image_size=final_image_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    label_mode='int',\n",
    "    seed=seed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_dir = \"MO2/classes\"\n",
    "batch_size = 64\n",
    "final_image_size = (309, 298)\n",
    "\n",
    "# Carica tutte le immagini dalle directory \"classes\" e \"augmented\"\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    image_size=final_image_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    label_mode='int',\n",
    "    seed=seed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "np.save('classes.npy', train_ds.class_names)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Combina i due dataset\n",
    "combined_dataset = train_ds.concatenate(train_ds2)\n",
    "del train_ds, train_ds2\n",
    "\n",
    "# Ottieni le etichette delle classi dal dataset\n",
    "#labels = np.concatenate([y for x, y in combined_dataset], axis=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "combined_dataset.class_names",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MvOOVqPt3Gh_",
    "outputId": "4669be59-f3d6-44dd-abd8-d9d83521e5ba"
   },
   "source": [
    "# Combina i due dataset\n",
    "combined_dataset = train_ds.concatenate(train_ds2)\n",
    "del train_ds, train_ds2\n",
    "\n",
    "# Ottieni le etichette delle classi dal dataset\n",
    "labels = np.concatenate([y for x, y in combined_dataset], axis=0)\n",
    "\n",
    "# Calcola i pesi delle classi\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "\n",
    "# Crea un dizionario per i pesi delle classi\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Salva il dizionario in un file JSON\n",
    "import json\n",
    "with open('class_weight_dict.json', 'w') as f:\n",
    "    json.dump(class_weight_dict, f)\n",
    "\n",
    "print(\"Il dizionario dei pesi delle classi Ã¨ stato salvato in 'class_weight_dict.json'.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "rqESaTvW3GiA",
    "outputId": "538edf42-dd54-4b19-fd6f-33ba16bcfd11"
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, lab in combined_dataset.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(labels[lab[i]])\n",
    "    plt.axis(\"off\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Definisci la dimensione del dataset\n",
    "dataset_size = sum(1 for _ in combined_dataset)\n",
    "\n",
    "# Definisci la proporzione per il training set\n",
    "train_size = int(0.8 * dataset_size)\n",
    "\n",
    "# Usa il metodo take per ottenere il training set\n",
    "train_dataset = combined_dataset.take(train_size)\n",
    "\n",
    "# Usa il metodo skip per ottenere il validation set\n",
    "validation_dataset = combined_dataset.skip(train_size)\n",
    "\n",
    "# Verifica il numero di immagini nei due dataset\n",
    "num_train_images = sum(1 for _ in train_dataset)\n",
    "num_validation_images = sum(1 for _ in validation_dataset)\n",
    "print(f\"Numero di immagini nel training set: {num_train_images}\")\n",
    "print(f\"Numero di immagini nel validation set: {num_validation_images}\")\n",
    "del combined_dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%load_ext jupyter_ai_magics",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%%ai chatgpt",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "validation_dataset = validation_dataset.map(lambda x, y: (normalization_layer(x), y))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LMjqZ10M3GiF"
   },
   "source": [
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class PlotLearning(Callback):\n",
    "    \"\"\"\n",
    "    Callback for plotting the learning curves of a model during training.\n",
    "\n",
    "    This callback records and visualizes training and validation metrics (e.g.,\n",
    "    loss, accuracy) after each epoch to provide insights into the model's\n",
    "    performance.\n",
    "\n",
    "    Methods:\n",
    "    - on_train_begin: Initializes a dictionary to store training metrics.\n",
    "    - on_epoch_end: Collects and stores metrics after each epoch and plots the\n",
    "    learning curves.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Initializes a dictionary to store training metrics\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "\n",
    "        # Plotting\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "\n",
    "        # Set up subplots\n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Convert axs to a numpy array for indexing\n",
    "        if not isinstance(axs, np.ndarray):\n",
    "            axs = np.array([axs])\n",
    "\n",
    "        # Plot learning curves\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, epoch + 2),\n",
    "                        self.metrics[metric],\n",
    "                        label=metric)\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, epoch + 2),\n",
    "                            self.metrics['val_' + metric],\n",
    "                            label='val_' + metric)\n",
    "\n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YXFgn2et3GiG"
   },
   "source": [
    "# Define the path pattern for saving checkpoints and extract the directory\n",
    "checkpoint_path = \"checkpoints/cp-{epoch:04d}.keras\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Configure a callback to save the best model based on validation accuracy\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Carica il modello pre-addestrato\n",
    "mm = fastervit.FasterViT0(pretrained=\"imagenet\", input_shape=(309, 298, 3))\n",
    "# Rimuovi l'ultimo livello del modello\n",
    "mm = tfk.Model(inputs=mm.input, outputs=mm.layers[-3].output)\n",
    "mm.trainable = False\n",
    "def build_fastervitv0():\n",
    "    \"\"\"\n",
    "    Build a FasterViT-based CNN with image augmentation, and a custom\n",
    "    classifier for multi-class classification.\n",
    "    \"\"\"\n",
    "    tf.random.set_seed(seed)\n",
    "    inputs = tf.keras.Input(shape=(309, 298, 3))\n",
    "\n",
    "    # Data Augmentation\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tfkl.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    ])\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    # Passa l'input attraverso il modello pre-addestrato senza la testa di classificazione\n",
    "    x1 = mm(x)\n",
    "    \n",
    "    # Applica GlobalAveragePooling2D per ridurre le dimensioni spaziali\n",
    "    x2 = tfkl.GlobalAveragePooling2D(name=\"avg_pool\")(x1)\n",
    "    norm = tfkl.BatchNormalization(name=\"batch_normalization\")(x2)\n",
    "\n",
    "    # Aggiungi un nuovo classificatore in cima al modello pre-addestrato\n",
    "    intermediate1 = tfkl.Dense(4096, activation=tf.keras.activations.swish)(norm)\n",
    "    batch = tfkl.BatchNormalization(name=\"batch_normalization_1\")(intermediate1)\n",
    "    drop1 = tfkl.Dropout(0.25)(batch)\n",
    "    intermediate2 = tfkl.Dense(2048, activation=tf.keras.activations.swish)(drop1)\n",
    "    batch = tfkl.BatchNormalization(name=\"batch_normalization_2\")(intermediate2)\n",
    "    outputs = tfkl.Dense(1800, activation='softmax', dtype='float32')(batch)\n",
    "\n",
    "    # Crea un modello collegando input e output\n",
    "    model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "    # Learning Rate Scheduler\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < 10:\n",
    "            return lr\n",
    "        else:\n",
    "            return lr * tf.math.exp(-0.1)\n",
    "\n",
    "    lr_scheduler = tfk.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    # Compila il modello con Sparse Categorical Cross-Entropy loss e Lion optimizer\n",
    "    model.compile(loss=tfk.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer=tfk.optimizers.Lion(),\n",
    "                  metrics=[\n",
    "                      tfk.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "                      tfk.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "                  ])\n",
    "    model.summary()\n",
    "\n",
    "    # Ritorna il modello e il learning rate scheduler\n",
    "    return model, lr_scheduler\n",
    "\n",
    "# Costruisci il modello\n",
    "tl_model, lr_scheduler = build_fastervitv0()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configura il callback per TensorBoard\n",
    "tensorboard_callback = tfk.callbacks.TensorBoard(log_dir='./logs')\n",
    "\n",
    "# Configura il callback per ridurre il learning rate on plateau\n",
    "reduce_lr = tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Configura il callback per EarlyStopping\n",
    "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)\n",
    "\n",
    "csv_logger = tfk.callbacks.CSVLogger('training.log', separator=\",\", append=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#tl_model.load_weights(\"new_model3/cp-0002.keras\")\n",
    "#tl_model = tfk.models.load_model(\"new_model4/cp-0004.keras\")\n",
    "#tl_model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train the model with transfer learning\n",
    "tl_history = tl_model.fit(\n",
    "    train_dataset, # input training data\n",
    "    validation_data=validation_dataset,\n",
    "    epochs = 50,\n",
    "    initial_epoch=25,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks = [early_stopping, lr_scheduler, tensorboard_callback, reduce_lr, csv_logger, cp_callback]\n",
    ").history"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "tl_model.save(\"tl_model.keras\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save weights of the Transfer Learning model\n",
    "#tl_model.save_weights('new.weights.h5')\n",
    "# Create a new instance of CustomModel for fine-tuning\n",
    "#ft_model = build_effnetv2()\n",
    "ft_model = tfk.models.load_model(\"new_FT2/cp-0001.keras\")\n",
    "# Load the weights for the model\n",
    "#ft_model.load_weights('new.weights.h5')\n",
    "\n",
    "# Set all layers as trainable\n",
    "ft_model.trainable = True\n",
    "\n",
    "if all(layer.trainable for layer in ft_model.layers):\n",
    "    print(\"Successful!\")\n",
    "else:\n",
    "    print(\"Error!\")\n",
    "\n",
    "ft_model.get_layer('batch_normalization').trainable = False\n",
    "ft_model.get_layer('batch_normalization_2').trainable = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tl_model.get_layer('batch_normalization').trainable = False\n",
    "tl_model.get_layer('batch_normalization_1').trainable = False\n",
    "tl_model.get_layer('batch_normalization_2').trainable = False\n",
    "tl_model.get_layer('dense_3').trainable = False\n",
    "tl_model.get_layer('dropout_1').trainable = False\n",
    "tl_model.get_layer('dense_4').trainable = False\n",
    "tl_model.get_layer('dense_5').trainable = False\n",
    "tl_model.get_layer('avg_pool').trainable = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Display the summary of the Transfer Learning model\n",
    "tl_model.summary()\n",
    "\n",
    "# Display the layers and their trainable status within the 'model_1' submodule\n",
    "print(\"Layers within 'FasterViT0' submodule:\")\n",
    "for i, layer in enumerate(tl_model.get_layer('model_1').layers): # To adjust\n",
    "    print(i, layer.name, layer.trainable)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display all layers in the entire model and their trainable status\n",
    "print(\"All layers in the model:\")\n",
    "for i, layer in enumerate(tl_model.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run this cell to run half of the layers.\n",
    "\n",
    "# Freeze the first N layers (approximately the first half) of the\n",
    "# 'convnext_large' submodule.\n",
    "# Also the fully-connected part will train.\n",
    "N = 535 # To adjust\n",
    "for i, layer in enumerate(tl_model.get_layer('model_1').layers[:N]):# To adjust\n",
    "    layer.trainable = True\n",
    "\n",
    "# Display the layers and their updated trainable status within the submodule\n",
    "print(\"Updated trainable status after freezing layers:\")\n",
    "for i, layer in enumerate(tl_model.get_layer('model_1').layers):# To adjust\n",
    "    print(i, layer.name, layer.trainable)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the path pattern for saving checkpoints and extract the directory\n",
    "checkpoint_path = \"FT2/cp-{epoch:04d}.keras\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Configure a callback to save the best model based on validation accuracy\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configura il callback per TensorBoard\n",
    "tensorboard_callback = tfk.callbacks.TensorBoard(log_dir='./logs_FT')\n",
    "\n",
    "# Configura il callback per ridurre il learning rate on plateau\n",
    "reduce_lr = tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=2, min_lr=1e-6)\n",
    "\n",
    "csv_logger = tfk.callbacks.CSVLogger('training_FT.log', separator=\",\", append=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Scongela tutti i layer del modello pre-addestrato (FasterViT0)\n",
    "for layer in tl_model.get_layer('model_1').layers:\n",
    "    layer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tl_model.get_layer('model_1').trainable = True",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verifica che i layer siano effettivamente scongelati\n",
    "for layer in tl_model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compila di nuovo il modello con un learning rate basso\n",
    "tl_model.compile(optimizer=tfk.optimizers.Lion(learning_rate=2.5e-6),\n",
    "                 loss=tfk.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=[\n",
    "                     tfk.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "                     tfk.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "                 ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tl_model.summary()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the model with transfer learning\n",
    "tl_history = tl_model.fit(\n",
    "    train_dataset, # input training data\n",
    "    validation_data=validation_dataset,\n",
    "    epochs = 5,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks = [tensorboard_callback, reduce_lr, csv_logger, cp_callback]\n",
    ").history"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T16:25:10.075700Z",
     "start_time": "2024-11-18T16:19:01.737298Z"
    }
   },
   "cell_type": "code",
   "source": "model = tfk.models.load_model(\"FT2/cp-0005.keras\")",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T16:27:31.190581Z",
     "start_time": "2024-11-18T16:25:36.156234Z"
    }
   },
   "cell_type": "code",
   "source": "tf.saved_model.save(model, os.path.join(os.getcwd(), \"fastervit/\"))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/penzo/workspace/MushDex/YOLO11/detection_training_final/fastervit/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/penzo/workspace/MushDex/YOLO11/detection_training_final/fastervit/assets\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:20:49.521395500Z",
     "start_time": "2024-11-18T16:40:11.377225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"fastervit/\")\n",
    "tflite_models_dir = pathlib.Path(\"tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "tflite_model_quant_file = tflite_models_dir/\"fastervit_model_quant.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_quant_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"/tf_lite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Carica il modello ONNX\n",
    "ort_session = ort.InferenceSession(\"tl_model.onnx\")\n",
    "\n",
    "# Carica l'immagine\n",
    "image = cv2.imread(\"4 (1).jpg\")\n",
    "\n",
    "# Ridimensiona l'immagine a 309x298\n",
    "image_resized = cv2.resize(image, (298, 309))\n",
    "\n",
    "# Converti l'immagine in float32 e normalizza i valori\n",
    "input_data = image_resized.astype(np.float32) / 255.0\n",
    "\n",
    "# Cambia l'ordine dei canali da HWC a CHW\n",
    "#input_data = np.transpose(input_data, (2, 0, 1))\n",
    "\n",
    "# Aggiungi una dimensione batch\n",
    "input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "# Ottieni il nome dell'input del modello\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "\n",
    "# Esegui l'inferenza\n",
    "results = ort_session.run(None, {input_name: input_data})\n",
    "\n",
    "# Ottieni le probabilitÃ  delle classi\n",
    "probabilities = results[0][0]\n",
    "\n",
    "# Ordina le probabilitÃ  e ottieni gli indici delle prime 5 classi\n",
    "top_5_indices = np.argsort(probabilities)[-5:][::-1]\n",
    "\n",
    "# Stampa i nomi delle classi con le probabilitÃ  piÃ¹ alte\n",
    "for i in top_5_indices:\n",
    "    print(f\"Classe: {train_ds.class_names[i]}, ProbabilitÃ : {probabilities[i]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ottieni il nome dell'input del modello\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "\n",
    "# Esegui l'inferenza\n",
    "results = ort_session.run(None, {input_name: input_data})\n",
    "\n",
    "# Stampa i risultati\n",
    "print(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e8xlPfKF3GiK"
   },
   "source": [
    "import onnxruntime as ort\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carica i nomi delle classi\n",
    "class_names = np.load('classes.npy')\n",
    "\n",
    "# Carica il modello ONNX\n",
    "ort_session = ort.InferenceSession(\"tl_model.onnx\")\n",
    "\n",
    "# Estrai le etichette vere e le etichette predette\n",
    "y_true = np.concatenate([y for x, y in validation_dataset], axis=0)\n",
    "y_pred_probs = []\n",
    "\n",
    "for x, y in validation_dataset:\n",
    "    # Prepara i dati di input\n",
    "    input_data = x.numpy().astype(np.float32)  # Converti in float32 invece di float16\n",
    "    \n",
    "    # Ottieni il nome dell'input del modello\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    \n",
    "    # Esegui l'inferenza\n",
    "    results = ort_session.run(None, {input_name: input_data})\n",
    "    \n",
    "    # Aggiungi i risultati alle probabilitÃ  predette\n",
    "    y_pred_probs.append(results[0])\n",
    "\n",
    "y_pred_probs = np.concatenate(y_pred_probs, axis=0)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Matrice di confusione\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Trova le 10 classi con piÃ¹ problemi (errori)\n",
    "errors = np.sum(conf_matrix, axis=1) - np.diag(conf_matrix)\n",
    "top_10_problematic_classes = np.argsort(errors)[-10:]\n",
    "\n",
    "# Visualizza la matrice di confusione per le 10 classi con piÃ¹ problemi\n",
    "conf_matrix_top_10 = conf_matrix[top_10_problematic_classes][:, top_10_problematic_classes]\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix_top_10, annot=True, cmap='Blues', fmt='g', xticklabels=[class_names[i] for i in top_10_problematic_classes], yticklabels=[class_names[i] for i in top_10_problematic_classes])\n",
    "plt.title('Confusion Matrix for Top 10 Problematic Classes')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Report di classificazione\n",
    "class_report = classification_report(y_true, y_pred_classes, target_names=class_names)\n",
    "print('Classification Report')\n",
    "print(class_report)\n",
    "\n",
    "# Accuratezza\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Precisione\n",
    "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
    "print(f'Precision: {precision * 100:.2f}%')\n",
    "\n",
    "# Richiamo\n",
    "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "print(f'Recall: {recall * 100:.2f}%')\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "print(f'F1 Score: {f1 * 100:.2f}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import some other useful libraries\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predict labels for the entire test set\n",
    "predictions = ft_model.predict(val_ds, verbose=0)\n",
    "\n",
    "# Display the shape of the predictions\n",
    "print(\"Predictions Shape:\", predictions.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract the true labels and predicted labels\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "#y_pred_probs = model.predict(val_ds)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "print('Confusion Matrix')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true, y_pred_classes)\n",
    "print('Classification Report')\n",
    "print(class_report)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
    "print(f'Precision: {precision * 100:.2f}%')\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "print(f'Recall: {recall * 100:.2f}%')\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "print(f'F1 Score: {f1 * 100:.2f}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Predicted probabilities for class 1 (unhealthy)\n",
    "predicted_probabilities = predictions[:,1]\n",
    "# True probabilities for class 1 (unhealthy)\n",
    "true_probabilities = y_true[:]\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(true_probabilities, predicted_probabilities)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Calculate Sensitivity (True Positive Rate) and Specificity (1 - False Positive Rate)\n",
    "sensitivity = tpr\n",
    "specificity = 1 - fpr\n",
    "\n",
    "# Calculate Youden's J statistic (tpr - fpr) for each threshold value\n",
    "youden = sensitivity + specificity - 1\n",
    "\n",
    "# Find the optimal threshold maximising the difference between tpr and fpr\n",
    "optimal_threshold = thresholds[np.argmax(youden)]\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal reference line\n",
    "\n",
    "# Annotate the optimal threshold with a star marker\n",
    "plt.scatter(fpr[np.argmax(youden)], tpr[np.argmax(youden)], marker='*',  color='black', s=100, label=f'Optimal Threshold = {optimal_threshold:.4f}', zorder=5)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "print(f'Optimal Threshold: {optimal_threshold}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def predict_given_threshold(prediction_probabilities, threshold= 0.5):\n",
    "  \"\"\"\n",
    "    Predict the class based on a threshold.\n",
    "    For each element in the 'probabilities' array, the function assigns class 1 ('unhealthy') if the\n",
    "    probability is greater than or equal to the threshold; otherwise, it assigns class 0 ('healthy').\n",
    "    The result is an array of predicted classes reflecting the classification decisions for each\n",
    "    corresponding probability in the input array.\n",
    "\n",
    "    Parameters:\n",
    "    - probabilities (array-like): Array or tensor of predicted probabilities of class 1.\n",
    "    - threshold (float): The threshold value for classification.\n",
    "\n",
    "    Returns:\n",
    "    - array: Array of predicted classes (1 or 0).\n",
    "  \"\"\"\n",
    "  return 1 * (prediction_probabilities[:,1] >= threshold)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute the new confusion matrix\n",
    "confusion_matrix_new =  skl.metrics.confusion_matrix(y_true, predict_given_threshold(predictions, optimal_threshold))\n",
    "\n",
    "# Compute classification metrics\n",
    "#accuracy = accuracy_score(np.argmax(y_true, axis=-1), predict_given_threshold(predictions, optimal_threshold))\n",
    "#precision = precision_score(np.argmax(y_true, axis=-1), predict_given_threshold(predictions, optimal_threshold), average='macro')\n",
    "#recall = recall_score(np.argmax(y_true, axis=-1), predict_given_threshold(predictions, optimal_threshold), average='macro')\n",
    "#f1 = f1_score(np.argmax(y_true, axis=-1), predict_given_threshold(predictions, optimal_threshold), average='macro')\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, predict_given_threshold(predictions, optimal_threshold))\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, predict_given_threshold(predictions, optimal_threshold), average='weighted')\n",
    "print(f'Precision: {precision * 100:.2f}%')\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, predict_given_threshold(predictions, optimal_threshold), average='weighted')\n",
    "print(f'Recall: {recall * 100:.2f}%')\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, predict_given_threshold(predictions, optimal_threshold), average='weighted')\n",
    "print(f'F1 Score: {f1 * 100:.2f}%')\n",
    "\n",
    "\"\"\"# Display the computed metrics\n",
    "print('Accuracy:', accuracy.round(4))\n",
    "print('Precision:', precision.round(4))\n",
    "print('Recall:', recall.round(4))\n",
    "print('F1:', f1.round(4))\"\"\"\n",
    "\n",
    "# Plot the confusion matrix\n",
    "#plt.figure(figsize=(10, 8))\n",
    "#sns.heatmap(confusion_matrix_new.T, cmap='Blues', annot= True, fmt='d')\n",
    "#plt.xlabel('True labels')\n",
    "#plt.ylabel('Predicted labels')\n",
    "#plt.show()\n",
    "# Plot confusion matrix\n",
    "#plot_confusion_matrix(cm, class_names)\n",
    "#plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "confusion_matrix_new"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "conf_matrix"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "optimal_threshold"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "img_path = 'test3.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "# Preprocess the image: Convert it to a numpy array and scale it\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "# If your model was trained on normalized images, you should normalize it here\n",
    "img_array = img_array / 127.5 - 1  # Normalize pixel values to [-1, 1] range\n",
    "#tl_model = tfk.models.load_model(\"new_model2/cp-0006.keras\")\n",
    "ft_model = tfk.models.load_model(\"new_FT2/cp-0003.keras\")\n",
    "# Predict the class\n",
    "predictions = ft_model.predict(img_array)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if (predictions[0][1]) > optimal_threshold:\n",
    "    print(\"Da Raccogliere\")\n",
    "if (predictions[0][1]) <= optimal_threshold:\n",
    "    print(\"Da Non Raccogliere\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
